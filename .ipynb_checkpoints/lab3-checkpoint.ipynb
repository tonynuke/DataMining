{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для\n",
      "ремонт\n",
      "коридор\n",
      "пост\n",
      "эц\n",
      "ст\n",
      "ферма\n",
      "необходимый\n",
      "изготовить\n",
      "и\n",
      "установить\n",
      "3\n",
      "противопожарный\n",
      "дверь\n",
      "в\n",
      "дверной\n",
      "проём\n"
     ]
    }
   ],
   "source": [
    "# исходная заявка\n",
    "sentence = '''Для ремонта коридора поста ЭЦ ст Ферма необходимо изготовить и установить 3 противопожарные двери в дверные проемы'''\n",
    "\n",
    "# разобъем ее на отдельные слова (токены)\n",
    "words = sentence.split(' ')\n",
    "\n",
    "# получим леммы (нормальная форма слова)\n",
    "for word in words:\n",
    "    result = morph.parse(word)\n",
    "    print(result[0].normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для PREP\n",
      "ремонта NOUN,inan,masc sing,gent\n",
      "коридора NOUN,inan,masc sing,gent\n",
      "поста NOUN,inan,masc sing,gent\n",
      "ЭЦ UNKN\n",
      "ст NOUN,inan,femn,Fixd,Abbr sing,nomn\n",
      "Ферма NOUN,inan,femn sing,nomn\n",
      "необходимо ADJS,Qual neut,sing\n",
      "изготовить INFN,perf,tran\n",
      "и CONJ\n",
      "установить INFN,perf,tran\n",
      "3 NUMB,intg\n",
      "противопожарные ADJF plur,nomn\n",
      "двери NOUN,inan,femn sing,gent\n",
      "в PREP\n",
      "дверные ADJF plur,nomn\n",
      "проемы NOUN,inan,masc plur,nomn\n"
     ]
    }
   ],
   "source": [
    "# получим всю инфу\n",
    "for word in words:\n",
    "    result = morph.parse(word)\n",
    "    print(word, result[0].tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ремонта коридора поста ЭЦ ст Ферма необходимо изготовить установить 3 противопожарные двери дверные проемы\n"
     ]
    }
   ],
   "source": [
    "def pos(word, morth):\n",
    "    return morth.parse(word)[0].tag.POS\n",
    "\n",
    "# удаляем вводные слова и т.п. (междометие, предлоги, частицы, союзы)\n",
    "functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP'}  # function words\n",
    "print(*[word for word in words if pos(word, morph) not in functors_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Для', 'ремонта', 'коридора', 'поста', 'ЭЦ', 'ст', 'Ферма', 'необходимо', 'изготовить', 'и', 'установить', '3', 'противопожарные', 'двери', 'в', 'дверные', 'проемы']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Для', 'PR'), ('ремонта', 'S'), ('коридора', 'S'), ('поста', 'S'), ('ЭЦ', 'S'), ('ст', 'S'), ('Ферма', 'S'), ('необходимо', 'PRAEDIC'), ('изготовить', 'V'), ('и', 'CONJ'), ('установить', 'V'), ('3', 'NUM=ciph'), ('противопожарные', 'A=pl'), ('двери', 'S'), ('в', 'PR'), ('дверные', 'A=pl'), ('проемы', 'S')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens, lang='rus')\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ремонта коридора поста ЭЦ ст Ферма необходимо изготовить установить 3 противопожарные двери дверные проемы\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "functors_pos = {'PR', 'CONJ'}  # function words\n",
    "print(*[word for word, pos in nltk.pos_tag(words, lang='rus') if pos not in functors_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('curious', 0.4), ('marvellous', 0.2), ('marvelous', 0.2), ('wondrous', 0.2)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Just to make it a bit more readable\n",
    "WN_NOUN = 'n'\n",
    "WN_VERB = 'v'\n",
    "WN_ADJECTIVE = 'a'\n",
    "WN_ADJECTIVE_SATELLITE = 's'\n",
    "WN_ADVERB = 'r'\n",
    "\n",
    "\n",
    "def convert(word, from_pos, to_pos):    \n",
    "    synsets = wn.synsets(word, pos=from_pos)\n",
    "\n",
    "    # Word not found\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Get all lemmas of the word (consider 'a'and 's' equivalent)\n",
    "    lemmas = []\n",
    "    for s in synsets:\n",
    "        for l in s.lemmas():\n",
    "            if s.name().split('.')[1] == from_pos or from_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and s.name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                lemmas += [l]\n",
    "\n",
    "    # Get related forms\n",
    "    derivationally_related_forms = [(l, l.derivationally_related_forms()) for l in lemmas]\n",
    "\n",
    "    # filter only the desired pos (consider 'a' and 's' equivalent)\n",
    "    related_noun_lemmas = []\n",
    "\n",
    "    for drf in derivationally_related_forms:\n",
    "        for l in drf[1]:\n",
    "            if l.synset().name().split('.')[1] == to_pos or to_pos in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE) and l.synset().name().split('.')[1] in (WN_ADJECTIVE, WN_ADJECTIVE_SATELLITE):\n",
    "                related_noun_lemmas += [l]\n",
    "\n",
    "    # Extract the words from the lemmas\n",
    "    words = [l.name() for l in related_noun_lemmas]\n",
    "    len_words = len(words)\n",
    "\n",
    "    # Build the result in the form of a list containing tuples (word, probability)\n",
    "    result = [(w, float(words.count(w)) / len_words) for w in set(words)]\n",
    "    result.sort(key=lambda w:-w[1])\n",
    "\n",
    "    # return all the possibilities sorted by probability\n",
    "    return result\n",
    "\n",
    "convert('wonder', 'n', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
